# -*- coding: utf-8 -*-
"""Attacks.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vJbOMPa2UIjZ2To193yrmAWJHwA4a41b
"""

# from google.colab import drive
# drive.mount('/content/drive/')

# !pip install cleverhans
# !pip install python_speech_features
# !pip3 install deepspeech

from tensorflow.python.keras.datasets import mnist
from tensorflow.python.keras.models import Sequential, Model
from tensorflow.python.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation
from tensorflow.python.keras.utils import to_categorical
# from tensorflow.python.keras.backend import *
from tensorflow.python.keras import backend as K
# import tensorflow.python as tf
import numpy as np
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()


trgt_inp = tf.placeholder(tf.float32, (10, 32, 32, 1))



# import tf.keras.backend as K
import math

(x_train, y_train), (x_test, y_test) = mnist.load_data()

batch_size = 32
epochs = 2

n_samples = x_train.shape[0]

img_width = x_train.shape[1]
img_height = x_train.shape[2]
channels = 1

y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

n_classes = y_train.shape[1]

print('Train size - {}. Each img {}x{} pixels.'.format(x_train.shape[0], img_width, img_height))

x_train = x_train.reshape((x_train.shape[0], img_width, img_height, 1))
x_test = x_test.reshape((x_test.shape[0], img_width, img_height, 1))

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')

x_train /= 255
x_test /= 255



model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(img_width, img_height, channels)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(n_classes))

sess = K._get_session()  # Get the current session from Keras.
def loss_fn(correct, predicted):
    return tf.nn.softmax_cross_entropy_with_logits(labels=correct,
                                                   logits=predicted)


model.compile(loss=loss_fn, optimizer='adam', metrics=['accuracy'])
model.summary()

model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_test, y_test))

"""FGSM ATTACK"""

n_adversarial = y_test.shape[0]

trgt_inp = tf.placeholder(tf.float32, (n_adversarial, img_width, img_height, 1))
true_out = tf.placeholder(tf.float32, (n_adversarial, 10))
epsilon = tf.placeholder(tf.float32, [n_adversarial, img_width, img_height, 1])

new_inp = tf.identity(trgt_inp)  #

output = model(new_inp)

loss = tf.sqrt(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=true_out))

grad = tf.gradients(loss, new_inp)
new_inp = tf.stop_gradient(new_inp + epsilon * tf.sign(grad))

step = 0.1
eps = np.full((n_adversarial, img_width, img_height, 1), step)

feed_dict = {trgt_inp: x_test,
             true_out: y_test,
             epsilon: eps}

x_adv, l = sess.run([new_inp, loss],
                    feed_dict)

x_adv = x_adv.reshape((n_adversarial, img_width, img_height, 1))

acc_bfr_attck = model.evaluate(x=x_test, y=y_test, batch_size=32)[1]
acc_aftr_attck = model.evaluate(x=x_adv, y=y_test, batch_size=32)[1]
print('Accuracy before FGSM {}, Accuracy after FGSM {}'.format(acc_bfr_attck, acc_aftr_attck))

# # note the accuracy depands on the step (epsilon) u chose, which is also effects
# # the perturbation to the picture
#
# import matplotlib.pyplot as plt
#
# plt.imshow(x_adv[0].squeeze(), cmap='gray')
# plt.show()
#
# test_samples = y_test.shape[0]
#
# n_batch = math.ceil(test_samples / batch_size)
# acc = 0
#
# inp_img = tf.placeholder(tf.float32, (None, img_width, img_height, channels))
# inp_y_true = tf.placeholder(tf.float32, (None, n_classes))
#
# y_pred = model(inp_img)
# y_pred = tf.argmax(y_pred, axis=1)
#
# y_true = tf.argmax(inp_y_true, axis=1)
#
# batch_acc = tf.equal(y_pred, y_true)
# batch_acc = tf.reduce_mean(tf.cast(batch_acc, tf.float32))
#
# for batch in range(n_batch):
#     start = batch * batch_size
#     end = min(start + batch_size, test_samples)
#
#     batch_acc_res = sess.run(batch_acc, feed_dict={inp_img: x_test[start:end],
#                                                    inp_y_true: y_test[start:end]})
#     acc += batch_acc_res * (end - start)
#
# acc /= test_samples
#
# # Sanity check
# keras_evaluation = model.evaluate(x=x_test, y=y_test, batch_size=batch_size)[1]
# our_evaluation = acc
#
# print(keras_evaluation)
# print(our_evaluation)
#
# n_target_samples = batch_size
#
# target_samples = []
# target_labels = []
#
# eye_mtrx = np.eye(n_classes)
#
# for sample_idx in range(n_target_samples):
#     for target_label in range(n_classes):
#         if target_label != np.argmax(y_test[sample_idx]):
#             target_samples.append(x_test[sample_idx])
#             target_labels.append(eye_mtrx[target_label])
#
# target_samples = np.array(target_samples)
# target_labels = np.array(target_labels)
#
# n_adversarial = target_samples.shape[0]
#
# shape = (n_adversarial, img_width, img_height, channels)
#
# trgt_img = tf.placeholder(tf.float32, shape)
# trgt_lbl = tf.placeholder(tf.float32, (n_adversarial, n_classes))
# const = tf.placeholder(tf.float32, [n_adversarial])
#
# perturbation = tf.Variable(np.zeros(shape), dtype=np.float32)
#
# new_img = tf.tanh(trgt_img + perturbation)
#
# logits = model(new_img)
#
# l2dist = tf.reduce_sum(tf.square(new_img - tf.tanh(trgt_img)), [1])
#
# trgt_lbl_conf = tf.reduce_sum(trgt_lbl * logits, 1)
# othr_lbl_conf = tf.reduce_max((1 - trgt_lbl) * logits - trgt_lbl * 10000, 1)
#
# loss1 = tf.maximum(0.0, othr_lbl_conf - trgt_lbl_conf)
# loss1 = tf.reduce_sum(const * loss1)
#
# loss2 = tf.reduce_sum(l2dist)
#
# loss = loss1 + loss2
#
# start_vars = set(x.name for x in tf.global_variables())
#
# optimizer = tf.train.AdamOptimizer(1e-2)
# train = optimizer.minimize(loss, var_list=[perturbation])
#
# end_vars = tf.global_variables()
#
# new_vars = [x for x in end_vars if x.name not in start_vars]
# new_vars.append(perturbation)
#
# sess.run(tf.variables_initializer(var_list=new_vars))
#
# const_inp = [10] * n_adversarial
#
# iterations = 10000
# for iteration in range(iterations):
#     _, l, l1, l2, l2d, scores, nimg = sess.run([train, loss, loss1, loss2, l2dist, logits, new_img],
#                                                {trgt_img: target_samples,
#                                                 trgt_lbl: target_labels,
#                                                 const: const_inp})
#     if iteration % (iterations // 10) == 0:
#         print(iteration, (l, l1, l2))
#
# lbls = np.argmax(scores, 1)
#
# import matplotlib.pyplot as plt
# from matplotlib.pyplot import figure
#
# fig = plt.figure(figsize=(3, 3))
# idx = 0
# fig.add_subplot(1, 2, 1)
# plt.imshow(nimg[idx].reshape([28, 28]))
# fig.add_subplot(1, 2, 2)
# plt.imshow(target_samples[idx].reshape([28, 28]))
#
# print('Adversary: {}'.format(lbls[idx]))
#
# """**HereBeDragons**"""
#
# !git
# clone
# https: // github.com / mozilla / DeepSpeech
#
# !wget
# https: // github.com / mozilla / DeepSpeech / releases / download / v0
# .4
# .1 / deepspeech - 0.4
# .1 - models.tar.gz
# !tar
# xvfz
# deepspeech - 0.4
# .1 - models.tar.gz
#
# # Commented out IPython magic to ensure Python compatibility.
# # %cd DeepSpeech
# !pip3
# install - r
# requirements.txt
# # pip3 install $(python3 util/taskcluster.py --decoder)
#
# !pip3
# install $(python3 util / taskcluster.py --decoder)
#
# import tensorflow as tf
# import os
# import sys
#
# sys.path.append('DeepSpeech')
#
# from util.flags import create_flags
#
# create_flags()
#
# model_path = 'models/output_graph.pb'
# graph_def = tf.GraphDef()
# graph_def.ParseFromString(open(model_path, 'rb').read())
#
# logits = tf.import_graph_def(graph_def,
#                              return_elements=['logits:0'])
#
# DeepSpeech.initialize_globals()
# # out = DeepSpeech.do_single_file_inference('../drive/My Drive/nice-work.wav')
#
# """***CLEVER HANS***"""
#
# !pip
# install
# cleverhans
#
# import cleverhans
# import keras
# import numpy as np
# from keras.datasets import mnist
# from keras.models import Sequential, Model
# from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation
# from keras.utils import to_categorical
#
# from cleverhans.utils_keras import KerasModelWrapper
# from cleverhans.attacks import CarliniWagnerL2
# import tensorflow as tf
#
# (x_train, y_train), (x_test, y_test) = mnist.load_data()
#
# batch_size = 32
# epochs = 1
#
# n_samples = x_train.shape[0]
#
# img_width = x_train.shape[1]
# img_height = x_train.shape[2]
# channels = 1
#
# y_train = to_categorical(y_train)
# y_test = to_categorical(y_test)
#
# n_classes = y_train.shape[1]
#
# x_train = x_train.reshape((x_train.shape[0], img_width, img_height, 1))
# x_test = x_test.reshape((x_test.shape[0], img_width, img_height, 1))
#
# x_train = x_train.astype('float32')
# x_test = x_test.astype('float32')
#
# x_train /= 255
# x_test /= 255
#
# sess = tf.Session()
# keras.backend.set_session(sess)
#
# model = Sequential()
# model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(img_width, img_height, channels)))
# model.add(Conv2D(64, (3, 3), activation='relu'))
# model.add(MaxPooling2D(pool_size=(2, 2)))
# model.add(Dropout(0.25))
# model.add(Flatten())
# model.add(Dense(128, activation='relu'))
# model.add(Dropout(0.5))
# model.add(Dense(n_classes, activation='softmax'))
#
# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
# model.summary()
#
# model.fit(x_train, y_train,
#           batch_size=batch_size,
#           epochs=epochs,
#           verbose=1,
#           validation_data=(x_test, y_test))
#
# wrap = KerasModelWrapper(model)
#
# cw = CarliniWagnerL2(wrap, sess=sess)
#
# cw_params = {'binary_search_steps': 1,
#              'max_iterations': 100,
#              'learning_rate': 0.05,
#              'batch_size': 32,
#              'initial_const': 10}
#
# adv = cw.generate_np(x_test[:32], **cw_params)
# adv.shape
#
# preds = np.argmax(model.predict(adv), axis=1)
#
# preds.shape
#
# import matplotlib.pyplot as plt
#
# idx = 0
# print(preds[idx])
# plt.imshow(adv[idx].reshape([28, 28]))
#
# plt.imshow(x_test[idx].reshape([28, 28]))
#
# """**REGRESSION NETWORK ATTACK**"""
#
# import pandas as pd
# import keras.backend as K
# import tensorflow as tf
# import numpy as np
#
# from keras.models import Sequential
# from keras.layers import Dense
#
# from sklearn.model_selection import train_test_split
# import matplotlib.pyplot as plt
#
# import os
#
# df = pd.read_csv('drive/My Drive/housing.txt', delim_whitespace=True, header=None)
# df = df.values
#
# x = df[:, 0:13]
# y = df[:, 13]
#
# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)
#
# sess = K.get_session()
#
# model = Sequential()
# model.add(Dense(13, input_shape=(13,), activation='relu'))
# model.add(Dense(1))
#
# model.compile(loss='mean_squared_error', optimizer='adam')
# model.summary()
#
# history = model.fit(x=x_train, y=y_train, batch_size=32, epochs=300, validation_split=0.1)
#
# plt.plot(history.history['loss'])
# plt.plot(history.history['val_loss'])
# plt.title('model loss')
# plt.ylabel('loss')
# plt.xlabel('epoch')
# plt.legend(['train', 'val'], loc='upper left')
# plt.show()
#
# model.evaluate(x_test, y_test)
#
# y_test = np.reshape(y_test, (y_test.shape[0], 1))
#
# n_adversarial = y_test.shape[0]
#
# trgt_inp = tf.placeholder(tf.float32, (n_adversarial, 13))
# true_out = tf.placeholder(tf.float32, (n_adversarial, 1))
# epsilon = tf.placeholder(tf.float32, [n_adversarial, 13])
#
# new_inp = tf.identity(trgt_inp)
#
# output = model(new_inp)
#
# loss = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(true_out, output))))
#
# grad = tf.gradients(loss, new_inp)
# new_inp = tf.stop_gradient(new_inp + epsilon * tf.sign(grad))
#
# iterations = 10
# step = 1
# eps = np.full((n_adversarial, 13), step)
#
# sess_inp = x_test
#
# for iteration in range(iterations):
#     x_adv, l = sess.run([new_inp, loss],
#                         {trgt_inp: sess_inp,
#                          true_out: y_test,
#                          epsilon: eps})
#     sess_inp = np.reshape(x_adv, (n_adversarial, 13))
#
#     if iteration % (iterations // 10) == 0:
#         print(iteration, l)
#
# from sklearn.metrics import mean_squared_error
#
# x_adv = np.reshape(x_adv, (n_adversarial, 13))
# mean_squared_error(model.predict(x_adv), y_test) ** 0.5
#
# mean_squared_error(model.predict(x_test), y_test) ** 0.5
#
# mean_squared_error(model.predict(x_test + np.full((n_adversarial, 13), 10)), y_test) ** 0.5
#
# mean_squared_error(x_adv, x_test) ** 0.5
#
# """***SOME MFCC SHIT***"""
#
# !pip
# install
# python_speech_features
# !git
# clone
# https: // github.com / mozilla / DeepSpeech.git
#
# from python_speech_features import mfcc
# import scipy.io.wavfile as wav
# import numpy as np
# import math
# import tensorflow as tf
# import os
# import sys
#
# features = mfcc(audio, samplerate=fs, numcep=32, winlen=0.032, winstep=0.02, winfunc=np.hamming)
# features.shape
#
# sess = tf.Session()
#
# samplerate = 16000
# winlen = 0.032
# winstep = 0.02
# numcep = 26
# framelen = 512
# framestep = 320
# preemphasis = 0.97
# numcontext = 9
# winfunc = tf.signal.hamming_window
#
# signal = audio
# nfft = 512
#
# # fbank
# # 1. preemphasis (signal, 0.97)
# highfreq = fs / 2
# signal = tf.cast(signal, dtype='float32')
# signal = tf.concat([signal[:1], signal[1:] - preemphasis * signal[:-1]], axis=0)
#
# # 2. framesig (signal, 512, 160, winfunc)
# numframes = 1 + tf.math.ceil((samplerate - framelen) / framestep)
# padlen = (numframes - 1) * framestep + framelen
# zeros = tf.zeros((padlen - samplerate,))
# padsignal = tf.concat((signal, zeros), axis=0)
#
# # rolling window
# frames = tf.stack([padsignal[i: i + framelen] for i in range(0, samplerate, framestep)])
# frames *= winfunc(framelen)
#
# # 3. powspec
# powspec = 1.0 / nfft * tf.square(tf.abs(tf.spectral.rfft(frames, [nfft])))
# energy = tf.reduce_sum(powspec, 1)
# energy = tf.where(energy == 0, energy + np.finfo(np.float).eps, energy)
# energy = tf.reshape(energy, [energy.shape[0], 1])
#
# # filterbanks
# from python_speech_features import get_filterbanks
#
# fb = get_filterbanks(nfilt=numcep, nfft=nfft, samplerate=samplerate)
# feat = tf.matmul(powspec, np.array(fb, dtype=np.float32).T)
# feat = tf.where(feat == 0, feat + np.finfo(np.float).eps, feat)
# feat = tf.log(feat)
#
# # dct
# feat = tf.spectral.dct(feat, type=2, norm='ortho')[:, :numcep]
#
# # lifter(feat, ceplifter)
# ceplifter = 22.
# n_frame, ncoeff = feat.shape
# n = tf.cast(tf.range(ncoeff), dtype='float32')
# lift = 1 + (ceplifter / 2.) * tf.sin(math.pi * n / ceplifter)
# feat = lift * feat
#
# # append energy
# feat = tf.concat([tf.log(energy), feat[:, 1:]], axis=1)
#
# tf_mfcc = sess.run(feat)
# tf_mfcc.shape
#
# from python_speech_features import *
#
# fs, audio = wav.read('drive/My Drive/sample.wav')
# real_mfcc = mfcc(audio, samplerate=fs, numcep=numcep, winlen=0.032, winstep=0.02, winfunc=np.hamming)
# real_mfcc.shape
#
# from sklearn.metrics import mean_squared_error
#
# mean_squared_error(tf_mfcc, real_mfcc)
#
# empty_context = tf.zeros((numcontext, numcep), dtype='float32')
# features = tf.concat((empty_context, tf_mfcc, empty_context), axis=0)
# features.shape
#
# """**BLOODY STRIDED**"""
#
# # !pip install python_speech_features
# import numpy as np
# import os
# from python_speech_features import mfcc
# import scipy.io.wavfile as wav
# import tensorflow as tf
#
# """Simple Example - List comprehension"""
#
# x = np.reshape(np.arange(2 * 3 * 4), (2, 3, 4))
# x
#
# x = np.reshape(x, [-1])
# x
#
# hopsize = 3
# framesize = 5
# x2 = np.array([x[i:i + framesize] for i in range(0, len(x) - framesize + 1, hopsize)])
# x2
#
# """Simple example - With strides"""
#
# window = framesize
# shape = x.shape[:-1] + (x.shape[-1] - framesize + 1, framesize)
# strides = x.strides + (x.strides[-1],)
# x2 = np.lib.stride_tricks.as_strided(x, shape=shape, strides=strides)[::hopsize]
# x2
#
# """Now for the action"""
#
# sample_path = 'drive/My Drive/sample.wav'
#
# fs, audio = wav.read(sample_path)
#
# features = mfcc(audio, samplerate=fs, numcep=32, winlen=0.032, winstep=0.02, winfunc=np.hamming)
#
# features.shape
#
# n_context = 9
# n_input = 26
# num_strides = len(features) - (n_context * 2)
# window_size = 2 * n_context + 1
#
# ftrs = np.lib.stride_tricks.as_strided(
#     features,
#     (num_strides, window_size, n_input),
#     (features.strides[0], features.strides[0], features.strides[1]),
#     writeable=False)
#
# ftrs.shape
#
# """How to convert the term above to list comprehension ??"""
#
# sess = tf.Session()
#
# features = tf.convert_to_tensor(features)
# features = tf.reshape(features, [-1])
#
# features = tf.stack([tf.reshape(features[i: i + window_size * n_input], (window_size, n_input))
#                      for i in range(0, features.shape[0] - window_size * n_input + 1, n_input)])
#
# tf_ftrs = sess.run(features)
#
# np.sum(ftrs - tf_ftrs)
#
# """**Universal Adversarial Perturbation Using C&W Attack**"""
#
# !git
# clone
# https: // github.com / LTS4 / universal
#
# # %cd universal
# # % cd python
# !python
# demo_inception.py - i
# data / test_img.png
#
# os.listdir()
#
# num_epochs = 100
# total_series_length = 50000
# truncated_backprop_length = 15
# state_size = 4
# num_classes = 2
# echo_step = 3
# batch_size = 5
# num_batches = total_series_length // batch_size // truncated_backprop_length
#
#
# def generateData():
#     x = np.array(np.random.choice(2, total_series_length, p=[0.5, 0.5]))
#     y = np.roll(x, echo_step)
#     y[0:echo_step] = 0
#
#     x = x.reshape((batch_size, -1))  # The first index changing slowest, subseries as rows
#     y = y.reshape((batch_size, -1))
#
#     return (x, y)
#
#
# x
#
