{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path - Running mode: 4\n"
     ]
    }
   ],
   "source": [
    "from DataLoader import *\n",
    "from Data import Data\n",
    "from Evalute import evaluate_model\n",
    "# from Models.SimpleCF import SimpleCF\n",
    "# from NeuMF import get_model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gast==0.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started...\n",
      "Dataset: movielens100k , convert_binary: True\n",
      "pre_processing.. \n",
      "n_real_users: 943 n_movies: 1682\n",
      "train_set size (/w negative sampling): 495285 test_set: size 943\n"
     ]
    }
   ],
   "source": [
    "convert_binary = True\n",
    "load_model = False\n",
    "testset_percentage = 0.2\n",
    "\n",
    "\n",
    "print('Started...')\n",
    "DATASET_NAME = 'movielens100k'\n",
    "# DATASET_NAME = 'movielens1m'\n",
    "df = get_from_dataset_name(DATASET_NAME, True)\n",
    "data = Data(seed=42)\n",
    "train_set, test_set, n_users, n_movies = data.pre_processing(df, test_percent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_fake_users = 300\n",
    "# # possible the need to have n_movies+1\n",
    "# fake_users_mat = np.zeros((n_fake_users , n_movies))\n",
    "# n_users_w_fake = n_fake_users + n_users + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lidora/opt/anaconda3/envs/ars/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/lidora/opt/anaconda3/envs/ars/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/lidora/opt/anaconda3/envs/ars/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/lidora/opt/anaconda3/envs/ars/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/lidora/opt/anaconda3/envs/ars/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/lidora/opt/anaconda3/envs/ars/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/lidora/opt/anaconda3/envs/ars/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/lidora/opt/anaconda3/envs/ars/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/lidora/opt/anaconda3/envs/ars/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/lidora/opt/anaconda3/envs/ars/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/lidora/opt/anaconda3/envs/ars/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/lidora/opt/anaconda3/envs/ars/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# import keras\n",
    "# from keras.layers import Embedding, Input, Flatten\n",
    "import tensorflow as tf\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, num_users, num_items, mf_dim=8, layers=[64, 32, 16, 8], reg_layers=[0, 0, 0, 0], reg_mf=0):\n",
    "        with tf.variable_scope('NeuCF_network'):\n",
    "            self.user_inp = tf.placeholder(dtype=tf.int32, shape=[None], name='user_input')\n",
    "            self.item_inp = tf.placeholder(dtype=tf.int32, shape=[None], name='item_input')\n",
    "            self.y_true = tf.placeholder(dtype=tf.float32, shape=[None], name='y_true')\n",
    "\n",
    "            normal_init = tf.random_normal_initializer(mean=0, stddev=0.01)\n",
    "            pred_init = tf.keras.initializers.lecun_uniform(seed=None)\n",
    "            # TODO: Add regulaizer\n",
    "            \n",
    "            mf_embedding_user = tf.Variable(normal_init([num_users, mf_dim]), name='mf_embedding_user')\n",
    "            mf_embed_user = tf.nn.embedding_lookup(mf_embedding_user, self.user_inp)\n",
    "\n",
    "            mf_embedding_item = tf.Variable(normal_init([num_items, mf_dim]), name='mf_embedding_item')\n",
    "            mf_embed_item = tf.nn.embedding_lookup(mf_embedding_item, self.item_inp)\n",
    "\n",
    "            mlp_embedding_user = tf.Variable(normal_init([num_users, int(layers[0] / 2)]), name='mlp_embedding_user')\n",
    "            mlp_embed_user = tf.nn.embedding_lookup(mlp_embedding_user, self.user_inp)\n",
    "\n",
    "            mlp_embedding_item = tf.Variable(normal_init([num_items, int(layers[0] / 2)]), name='mlp_embedding_item')\n",
    "            mlp_embed_item = tf.nn.embedding_lookup(mlp_embedding_item, self.item_inp)\n",
    "            \n",
    "            # MF part\n",
    "            mf_user_latent = tf.reshape(mf_embed_user, [-1, mf_embed_user.shape[-1]])\n",
    "            mf_item_latent = tf.reshape(mf_embed_item, [-1, mf_embed_item.shape[-1]])\n",
    "            mf_vector = tf.multiply(mf_user_latent, mf_item_latent)\n",
    "            # MLP part\n",
    "            mlp_user_latent = tf.reshape(mlp_embed_user, [-1, mlp_embed_user.shape[-1]])\n",
    "            mlp_item_latent = tf.reshape(mlp_embed_item, [-1, mlp_embed_item.shape[-1]])\n",
    "            mlp_vector = tf.concat([mlp_user_latent, mlp_item_latent], axis=-1)\n",
    "            num_layer = len(layers)\n",
    "            for idx in range(1, num_layer):\n",
    "                mlp_vector = tf.layers.dense(mlp_vector, layers[idx], activation=tf.nn.relu, name=\"layer{}\".format(idx))\n",
    "\n",
    "            self.predict_vector = tf.concat([mf_vector, mlp_vector], axis =-1)\n",
    "            \n",
    "            prediction = tf.layers.dense(inputs=self.predict_vector,\n",
    "                                              kernel_initializer = pred_init,\n",
    "                                              bias_initializer = pred_init,\n",
    "                                              units=1, activation='sigmoid')\n",
    "            self.prediction = tf.reshape(prediction, [-1], name='prediction')\n",
    "            self.loss = tf.keras.losses.binary_crossentropy(y_pred=self.prediction, y_true=self.y_true)\n",
    "#             self.loss = tf.reduce_mean((self.y_true*tf.log(self.prediction)) + ((1-self.y_true)*tf.log(1-self.prediction))) # (Y_TRUE - PREDICTION)\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "            self.train_op = self.optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(n_fake_users, n_users, n_movies):\n",
    "    num_users = n_fake_users + n_users\n",
    "    num_items = n_movies\n",
    "    batch_size = 512\n",
    "    \n",
    "    #model params:\n",
    "    mf_dim = 8\n",
    "    layers = [64, 32, 16, 8]\n",
    "    reg_layers = [0, 0, 0, 0]\n",
    "    reg_mf = 0\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    model = Model(num_users = num_users,\n",
    "                  num_items = num_items,\n",
    "                  mf_dim = mf_dim,\n",
    "                  layers= layers,\n",
    "                  reg_layers=reg_layers,\n",
    "                  reg_mf=reg_mf)\n",
    "    print(f'model with: n_fake_users={n_fake_users}, n_users={n_users}, n_movies={n_movies} created')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFPredictWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    def predict(self, user_item, verbose):\n",
    "        [u, i] = user_item\n",
    "        model = self.model\n",
    "        preds = sess.run(model.prediction,\n",
    "                          feed_dict={model.user_inp: u,\n",
    "                           model.item_inp: i})\n",
    "                                            \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-fe922eb3d1c4>:38: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /Users/lidora/opt/anaconda3/envs/ars/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/lidora/opt/anaconda3/envs/ars/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "model with: n_fake_users=8, n_users=943, n_movies=1682 created\n",
      "e=1 hr=0.453 ndcg=0.251 time=0.300 train_loss=0.402 \n",
      "e=2 hr=0.507 ndcg=0.269 time=0.277 train_loss=0.361 \n",
      "e=3 hr=0.513 ndcg=0.275 time=0.284 train_loss=0.344 \n",
      "e=4 hr=0.522 ndcg=0.282 time=0.278 train_loss=0.334 \n",
      "e=5 hr=0.520 ndcg=0.276 time=0.277 train_loss=0.327 \n",
      "e=6 hr=0.527 ndcg=0.283 time=0.276 train_loss=0.321 \n",
      "e=7 hr=0.523 ndcg=0.281 time=0.292 train_loss=0.317 \n",
      "e=8 hr=0.522 ndcg=0.280 time=0.281 train_loss=0.313 \n",
      "e=9 hr=0.524 ndcg=0.283 time=0.287 train_loss=0.309 \n",
      "e=10 hr=0.531 ndcg=0.284 time=0.238 train_loss=0.305 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "N_FAKE_USERS = 8\n",
    "# https://stackoverflow.com/questions/49842705/reuse-tensorflow-session-without-making-a-checkpoint\n",
    "\n",
    "def get_batches(training_set, batch_size ):\n",
    "    training_mat = np.column_stack(training_set)\n",
    "    np.random.shuffle(training_mat)\n",
    "    # return np.vsplit(training_mat, training_mat.shape[0] // batch_size)\n",
    "    batches = np.array_split(training_mat, training_mat.shape[0] // batch_size)\n",
    "    return batches\n",
    "    # print(len(batches))\n",
    "epochs = 10\n",
    "batch_size = 512\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    model = get_model(n_fake_users=N_FAKE_USERS, n_users=n_users, n_movies=n_movies)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    model_p = TFPredictWrapper(model)\n",
    "    # training and validation\n",
    "    for e in range(epochs):\n",
    "        batches = get_batches(train_set, batch_size = batch_size)\n",
    "        loses_in_epoch = []\n",
    "        for b in batches:\n",
    "            u = b[:,0]\n",
    "            i = b[:,1]\n",
    "            r = b[:,2]\n",
    "            _, preds, loss = sess.run([model.train_op, model.prediction, model.loss],\n",
    "                                      feed_dict={model.user_inp: u,\n",
    "                                       model.item_inp: i,\n",
    "                                       model.y_true: r})\n",
    "            loses_in_epoch.append(np.mean(loss))\n",
    "        epoch_loss = np.mean(loses_in_epoch)\n",
    "\n",
    "        mean_hr, mean_ndcg, time_eval = evaluate_model(model_p, test_set, verbose=0)\n",
    "        print('e={} hr={:.3f} ndcg={:.3f} time={:.3f} train_loss={:.3f} '.format(e+1, mean_hr, mean_ndcg, time_eval, epoch_loss))\n",
    "    save_path = saver.save(sess, \"model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model with: n_fake_users=8, n_users=943, n_movies=1682 created\n",
      "WARNING:tensorflow:From /Users/lidora/opt/anaconda3/envs/ars/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from model.ckpt\n",
      "e=10 hr=0.531 ndcg=0.284 time=0.322\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    model = get_model(n_fake_users=N_FAKE_USERS, n_users=n_users, n_movies=n_movies)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, \"model.ckpt\")\n",
    "    model_p = TFPredictWrapper(model)\n",
    "    mean_hr, mean_ndcg, time_eval = evaluate_model(model_p, test_set, verbose=0)\n",
    "    print('e={} hr={:.3f} ndcg={:.3f} time={:.3f}'.format(e+1, mean_hr, mean_ndcg, time_eval))\n",
    "#     preds = model.sess.run(model.prediction,\n",
    "#                               feed_dict={model.user_inp: users,\n",
    "#                                model.item_inp: items})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack rating size= 11767\n",
      "real_training_rating_size= 495285\n"
     ]
    }
   ],
   "source": [
    "# mal_train_set = [users + train_set[0], items + train_set[1]]\n",
    "output_dim = (N_FAKE_USERS-1) * (n_movies-1)\n",
    "fake_ratings = np.full((output_dim,), 2)\n",
    "cartesian_prodcut = np.array([[n_users + user - 1, item] \n",
    "                              for user in np.arange(1, N_FAKE_USERS) for item in np.arange(1, n_movies)])\n",
    "attack_users = cartesian_prodcut[:, 0]\n",
    "attack_items = cartesian_prodcut[:, 1]\n",
    "# np.append(fake_ratings, train_set[2])\n",
    "attack_train_set = np.array([np.append(attack_users, train_set[0]),\n",
    "                          np.append(attack_items, train_set[1])])\n",
    "real_training_rating_size = len(train_set[2])\n",
    "print('attack rating size=', output_dim)\n",
    "print('real_training_rating_size=', real_training_rating_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model with: n_fake_users=8, n_users=943, n_movies=1682 created\n",
      "(507052,)\n",
      "INFO:tensorflow:Restoring parameters from model.ckpt\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mal_train_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3f07df748cb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mnew_labels_mal_part\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mnew_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_labels_mal_part\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mall_users\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmal_train_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mall_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmal_train_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_users\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mal_train_set' is not defined"
     ]
    }
   ],
   "source": [
    "output_dim = (N_FAKE_USERS-1) * (n_movies-1)\n",
    "tf.reset_default_graph()\n",
    "mal_real_shape = output_dim + real_training_rating_size\n",
    "rating_input = tf.placeholder(tf.float32, [mal_real_shape], name='benzona')\n",
    "model = get_model(n_fake_users=N_FAKE_USERS, n_users=n_users, n_movies=n_movies)\n",
    "print(rating_input.shape)\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, \"model.ckpt\")\n",
    "    eps_p = tf.placeholder(tf.float32, mal_real_shape)\n",
    "    mask_p = tf.placeholder(tf.float32, mal_real_shape)\n",
    "#     prediction = tf.layers.dense(inputs=model.predict_vector,units=1, activation='sigmoid')\n",
    "#     prediction = tf.reshape(prediction, [-1], name='prediction')\n",
    "\n",
    "    # look for an option to limit amount of poison\n",
    "    theta = 1\n",
    "    # could use a hinge loss - mean below average will not tolerate.\n",
    "    # reg might need to be negative, loss itself positive and increasing\n",
    "    # rint will round those under 0.5, while higher will count as loss.\n",
    "#     reg_loss = -theta*tf.reduce_mean(rating_input*mask_p) # theta=3.5\n",
    "    reg_loss = -theta*(tf.reduce_sum(tf.rint(rating_input * mask_p)) / N_FAKE_USERS)\n",
    "    obj_loss = tf.keras.losses.binary_crossentropy(y_pred=model.prediction, y_true=rating_input)\n",
    "    combined_loss = reg_loss + obj_loss\n",
    "    grad = tf.gradients(combined_loss, rating_input) # take the gradient of the loss according to prediction # check\n",
    "    # new_labels_op = tf.stop_gradient(rating_input + epsilon * tf.sign(grad)) # perform a gradient step\n",
    "#     rating_input_out = tf.stop_gradient(rating_input + (eps_p * tf.sign(grad) * mask_p)) # perform a gradient step\n",
    "    rating_input_out = tf.stop_gradient(rating_input + (eps_p * grad * mask_p)) # perform a gradient step\n",
    "    rating_input_out = tf.reshape(tf.clip_by_value(rating_input_out, 0, 1), (-1,))\n",
    "    step = 1000\n",
    "    eps = np.full((mal_real_shape,), step)\n",
    "#     y_true = np.random.randint(0, 2, (output_dim,)) ### WHATT IS THIS?\n",
    "    # try to emulate a session with all data - training_set + adver, then the gradient will adjust adver rating params\n",
    "#     new_labels = np.full((output_dim,), 0.01) # initiate new ratings at 0.5\n",
    "    new_labels_mal_part = np.full((output_dim,), 0.5)\n",
    "    new_labels = np.concatenate([new_labels_mal_part, train_set[2]])\n",
    "    all_users = mal_train_set[0]\n",
    "    all_items = mal_train_set[1]\n",
    "    indexes = np.arange(len(all_users))\n",
    "#new_labels.shape\n",
    "    for i in range(50):\n",
    "        ## Todo: There is an issue in creating a shuffle while keeping track in the order of the mal data\n",
    "#         print(i)\n",
    "#         p = np.random.permutation(len(new_labels))\n",
    "#         all_users = all_users[p]\n",
    "#         all_items = all_items[p]\n",
    "#         new_labels = new_labels[p]\n",
    "#         new_labels_before_change = new_labels.copy()\n",
    "        mask = np.zeros((mal_real_shape,))\n",
    "#         indexes = indexes[p] # this will help get the mal ratings in order as we started\n",
    "        mask[np.argwhere(indexes < output_dim)] = 1\n",
    "        \n",
    "        new_labels, adv_l, obj_l, reg_l= sess.run([rating_input_out, combined_loss, obj_loss, reg_loss],\n",
    "                                 feed_dict={model.user_inp: all_users,\n",
    "                                            model.item_inp: all_items,\n",
    "                                            rating_input: new_labels,\n",
    "                                            mask_p: mask,\n",
    "                                            eps_p: eps})\n",
    "        new_labels = new_labels.reshape((-1,))\n",
    "#         new_labels[np.argwhere(indexes >= output_dim)] = new_labels_before_change[np.argwhere(indexes >= output_dim)]\n",
    "        attack_rating = new_labels[np.argwhere(indexes < output_dim)][np.arange(output_dim)].reshape(-1,)\n",
    "        ## Safety Check such that legit ratings did not change\n",
    "        legit_rating = new_labels[np.argwhere(indexes > output_dim)].reshape(-1,)\n",
    "        assert len(np.unique(legit_rating)) ==2, 'legit rating must not change during gradient step'\n",
    "        print(i, np.round(attack_rating[:10], 2), np.mean(attack_rating), adv_l, obj_l, reg_l)\n",
    "#         print('adv_l={:0.4f}, max={:.4f} min={:.4f} count={}'.format(adv_l, np.max(mal_only), np.min(mal_only), len(np.unique(mal_only))))\n",
    "#         new_labels_before_change[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x136fb4550>"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for each user, we want to keep all positive ones, and keep excatly positive_ones* 4 zeros.\n",
    "assert len(attack_users) == len(attack_items)\n",
    "assert len(attack_users) == len(attack_rating)\n",
    "import pandas as pd\n",
    "base_attack_df = pd.DataFrame({'u': attack_users, 'i':attack_items, 'r': attack_rating})\n",
    "# maybe its better to take top ## instead of rint\n",
    "base_attack_df.r.hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lidora/opt/anaconda3/envs/ars/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>i</th>\n",
       "      <th>r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>943</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>943</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>943</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>943</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>943</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11762</th>\n",
       "      <td>949</td>\n",
       "      <td>1677</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11763</th>\n",
       "      <td>949</td>\n",
       "      <td>1678</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11764</th>\n",
       "      <td>949</td>\n",
       "      <td>1679</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11765</th>\n",
       "      <td>949</td>\n",
       "      <td>1680</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11766</th>\n",
       "      <td>949</td>\n",
       "      <td>1681</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8081 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         u     i  r\n",
       "1      943     2  1\n",
       "14     943    15  1\n",
       "17     943    18  1\n",
       "18     943    19  1\n",
       "19     943    20  1\n",
       "...    ...   ... ..\n",
       "11762  949  1677  1\n",
       "11763  949  1678  1\n",
       "11764  949  1679  1\n",
       "11765  949  1680  1\n",
       "11766  949  1681  1\n",
       "\n",
       "[8081 rows x 3 columns]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df[:].groupby('u').apply(lambda x: print(x))\n",
    "treshold = 0.5\n",
    "attack_df_filterd = base_attack_df.copy()\n",
    "# attack_df_filterd['r'] = attack_df_filterd['r'].apply(lambda x: np.rint(x))\n",
    "attack_df_filterd = base_attack_df[base_attack_df.apply(lambda x: x.r > treshold, axis=1)] # apply on rows, keep only high prob ratings\n",
    "attack_df_filterd['r'] = attack_df_filterd['r'].apply(lambda x: 1) #|.astype(int)\n",
    "attack_df_filterd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 951.74it/s]\n",
      "100%|██████████| 8081/8081 [00:01<00:00, 6769.48it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([943, 943, 943, ..., 949, 949, 949]),\n",
       " array([1005,  105,  865, ..., 1350, 1414,  352]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]))"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## add negative sampling to the mal data frame\n",
    "from tqdm import tqdm\n",
    "df = attack_df_filterd\n",
    "negative_items = {}\n",
    "users = list(sorted(df['u'].unique()))\n",
    "for idx, user in tqdm(enumerate(users), total=len(users)):\n",
    "    negative_items[user] = df[df['u'] != user]['i'].unique()\n",
    "negative_items\n",
    "num_negatives = 4\n",
    "df = df.groupby('u').apply(lambda s: s.sample(frac=1))\n",
    "user_input, item_input, labels = [], [], []\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    user = row['u']\n",
    "    user_input.append(user)\n",
    "    item_input.append(row['i'])\n",
    "    labels.append(row['r'])\n",
    "    negative_input_items = np.random.choice(negative_items[user], num_negatives)\n",
    "    for neg_item in negative_input_items:\n",
    "        user_input.append(user)\n",
    "        item_input.append(neg_item)\n",
    "        labels.append(0)\n",
    "\n",
    "mal_training_set = (np.array(user_input), np.array(item_input), np.array(labels))\n",
    "# mal_training_set = (df['u'].values, df['i'].values, df['r'].values)\n",
    "mal_training_set # this already has the inputs and negative items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_filterd 7188\n",
      "train_set 495285\n",
      "58055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "107583"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Data import create_subset, concat_and_shuffle\n",
    "TRAIN_FRAC = 0.10\n",
    "malicious_training_set = mal_training_set\n",
    "# malicious_training_set = (df_filterd.values[:,0], df_filterd.values[:,1], df_filterd.values[:,2])\n",
    "print('df_filterd', len(df_filterd))\n",
    "print('train_set',len(train_set[0]))\n",
    "train_set_subset = create_subset(train_set, train_frac=TRAIN_FRAC)\n",
    "attack_benign_training_set = concat_and_shuffle(malicious_training_set, train_set_subset)\n",
    "print(len(malicious_training_set[0]))\n",
    "len(attack_benign_training_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model with: n_fake_users=8, n_users=943, n_movies=1682 created\n",
      "INFO:tensorflow:Restoring parameters from model.ckpt\n",
      "0.302 0.532 0.278 0.310\n",
      "0.302 0.533 0.280 0.255\n",
      "0.302 0.522 0.274 0.260\n",
      "0.302 0.515 0.269 0.269\n",
      "0.302 0.491 0.256 0.255\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model using the fake predictions\n",
    "tf.reset_default_graph()\n",
    "model = get_model(n_fake_users=N_FAKE_USERS, n_users=n_users, n_movies=n_movies)\n",
    "model_p = TFPredictWrapper(model)\n",
    "mal_epochs = 5\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, \"model.ckpt\")\n",
    "    for e in range(mal_epochs):\n",
    "        batches = get_batches(attack_benign_training_set, batch_size = batch_size)\n",
    "        u = attack_benign_training_set[0]\n",
    "        i = attack_benign_training_set[1]\n",
    "        r = attack_benign_training_set[2]\n",
    "        _, preds, loss = sess.run([model.train_op, model.prediction, model.loss],\n",
    "                                  feed_dict={model.user_inp: u,\n",
    "                                   model.item_inp: i,\n",
    "                                   model.y_true: r})\n",
    "            \n",
    "        epoch_loss = np.mean(loses_in_epoch)\n",
    "        mean_hr, mean_ndcg, time_eval = evaluate_model(model_p, test_set, verbose=0)\n",
    "        print('{:.3f} {:.3f} {:.3f} {:.3f}'.format(epoch_loss, mean_hr, mean_ndcg, time_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model with: n_fake_users=8, n_users=943, n_movies=1682 created\n",
      "INFO:tensorflow:Restoring parameters from model.ckpt\n",
      "0.421 0.538 0.282 0.307\n",
      "0.399 0.531 0.279 0.282\n",
      "0.390 0.520 0.275 0.282\n",
      "0.382 0.504 0.265 0.280\n",
      "0.373 0.490 0.261 0.278\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model using the fake predictions\n",
    "tf.reset_default_graph()\n",
    "model = get_model(n_fake_users=N_FAKE_USERS, n_users=n_users, n_movies=n_movies)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, \"model.ckpt\")\n",
    "    model_p = TFPredictWrapper(model)\n",
    "    for e in range(mal_epochs):\n",
    "        batches = get_batches(attack_benign_training_set, batch_size = batch_size)\n",
    "        loses_in_epoch = []\n",
    "        for b in batches:\n",
    "            u = b[:,0]\n",
    "            i = b[:,1]\n",
    "            r = b[:,2]\n",
    "            _, preds, loss = sess.run([model.train_op, model.prediction, model.loss],\n",
    "                                      feed_dict={model.user_inp: u,\n",
    "                                       model.item_inp: i,\n",
    "                                       model.y_true: r})\n",
    "            loses_in_epoch.append(np.mean(loss))\n",
    "        epoch_loss = np.mean(loses_in_epoch)\n",
    "        mean_hr, mean_ndcg, time_eval = evaluate_model(model_p, test_set, verbose=0)\n",
    "        print('{:.3f} {:.3f} {:.3f} {:.3f}'.format(epoch_loss, mean_hr, mean_ndcg, time_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_inp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-240-d7611adf3e2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# with model.sess:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnew_inp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'new_inp' is not defined"
     ]
    }
   ],
   "source": [
    "# with model.sess:\n",
    "new_inp.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(1, n_fake_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"FGSM ATTACK\"\"\"\n",
    "\n",
    "# user_inp = tf.placeholder(tf.float32, (n_fake_users, 64))\n",
    "# item_inp = tf.placeholder(tf.float32, (n_movies, 64))\n",
    "# # true_out = tf.placeholder(tf.float32, (n_fake_users, 10))\n",
    "epsilon = tf.placeholder(tf.float32, [n_fake_users * n_movies])\n",
    "# user_inp = tf.Variable((n_fake_users, 64), tf.float32)\n",
    "# item_inp = tf.Variable((n_fake_users, 64), tf.float32)\n",
    "# # true_out = tf.placeholder(tf.float32, (n_fake_users, 10))\n",
    "# epsilon = tf.Variable([n_fake_users, 64], tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    mean_hr, mean_ndcg, time_eval = evaluate_model(model_p, test_set, verbose=0)\n",
    "    preds = sess.run(model.prediction,\n",
    "                              feed_dict={model.user_inp: users,\n",
    "                               model.item_inp: items})\n",
    "    grad = tf.gradients(model.loss, [self.rating_input])\n",
    "# grad\n",
    "# grad1 = tf.gradients(loss, new_inp)\n",
    "# grad = tf.where(tf.is_nan(grad1), tf.zeros_like(grad1)*1, grad1) #replace nans with zeros\n",
    "# new_inp = tf.stop_gradient(data + epsilon * tf.sign(grad))\n",
    "# new_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"FGSM ATTACK\"\"\"\n",
    "\n",
    "n_adversarial = y_test.shape[0]\n",
    "\n",
    "trgt_inp = tf.placeholder(tf.float32, (n_fake_users, 64))\n",
    "# true_out = tf.placeholder(tf.float32, (n_fake_users, 10))\n",
    "epsilon = tf.placeholder(tf.float32, [n_adversarial, img_width, img_height, 1])\n",
    "\n",
    "new_inp = tf.identity(trgt_inp)  #returns a tensor with the same shape and type\n",
    "\n",
    "output = model(new_inp)\n",
    "\n",
    "loss = tf.sqrt(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=true_out))\n",
    "\n",
    "grad = tf.gradients(loss, new_inp)\n",
    "new_inp = tf.stop_gradient(new_inp + epsilon * tf.sign(grad))\n",
    "\n",
    "step = 0.1\n",
    "eps = np.full((n_adversarial, img_width, img_height, 1), step)\n",
    "\n",
    "feed_dict = {trgt_inp: x_test,\n",
    "             true_out: y_test,\n",
    "             epsilon: eps}\n",
    "\n",
    "x_adv, l = sess.run([new_inp, loss],\n",
    "                    feed_dict)\n",
    "\n",
    "x_adv = x_adv.reshape((n_adversarial, img_width, img_height, 1))\n",
    "\n",
    "acc_bfr_attck = model.evaluate(x=x_test, y=y_test, batch_size=32)[1]\n",
    "acc_aftr_attck = model.evaluate(x=x_adv, y=y_test, batch_size=32)[1]\n",
    "print('Accuracy before FGSM {}, Accuracy after FGSM {}'.format(acc_bfr_attck, acc_aftr_attck))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[2.]]\n",
    "m = tf.matmul(x, x)\n",
    "print(\"hello, {}\".format(m))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.zeros((10,))\n",
    "indexes = [1, 3, 5 ,7]\n",
    "t[indexes] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[indexes] = not t[indexes].all\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.43379653859233"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 0.000000000009\n",
    "-(1*np.log(p) + 0 * np.log(1-p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
