{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataLoader import *\n",
    "from Data import Data\n",
    "from Evalute import evaluate_model\n",
    "# from Models.SimpleCF import SimpleCF\n",
    "# from NeuMF import get_model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gast==0.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_binary = True\n",
    "load_model = False\n",
    "testset_percentage = 0.2\n",
    "\n",
    "\n",
    "print('Started...')\n",
    "DATASET_NAME = 'movielens100k'\n",
    "# DATASET_NAME = 'movielens1m'\n",
    "df = get_from_dataset_name(DATASET_NAME, True)\n",
    "data = Data(seed=42)\n",
    "train_set, test_set, n_users, n_movies = data.pre_processing(df, test_percent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_fake_users = 300\n",
    "# # possible the need to have n_movies+1\n",
    "# fake_users_mat = np.zeros((n_fake_users , n_movies))\n",
    "# n_users_w_fake = n_fake_users + n_users + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras\n",
    "# from keras.layers import Embedding, Input, Flatten\n",
    "import tensorflow as tf\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, num_users, num_items, mf_dim=8, layers=[64, 32, 16, 8], reg_layers=[0, 0, 0, 0], reg_mf=0):\n",
    "        with tf.variable_scope('NeuCF_network'):\n",
    "            self.user_inp = tf.placeholder(dtype=tf.int32, shape=[None], name='user_input')\n",
    "            self.item_inp = tf.placeholder(dtype=tf.int32, shape=[None], name='item_input')\n",
    "            self.y_true = tf.placeholder(dtype=tf.float32, shape=[None], name='y_true')\n",
    "\n",
    "            normal_init = tf.random_normal_initializer(mean=0, stddev=0.01)\n",
    "            pred_init = tf.keras.initializers.lecun_uniform(seed=None)\n",
    "            # TODO: Add regulaizer\n",
    "            \n",
    "            mf_embedding_user = tf.Variable(normal_init([num_users, mf_dim]), name='mf_embedding_user')\n",
    "            mf_embed_user = tf.nn.embedding_lookup(mf_embedding_user, self.user_inp)\n",
    "\n",
    "            mf_embedding_item = tf.Variable(normal_init([num_items, mf_dim]), name='mf_embedding_item')\n",
    "            mf_embed_item = tf.nn.embedding_lookup(mf_embedding_item, self.item_inp)\n",
    "\n",
    "            mlp_embedding_user = tf.Variable(normal_init([num_users, int(layers[0] / 2)]), name='mlp_embedding_user')\n",
    "            mlp_embed_user = tf.nn.embedding_lookup(mlp_embedding_user, self.user_inp)\n",
    "\n",
    "            mlp_embedding_item = tf.Variable(normal_init([num_items, int(layers[0] / 2)]), name='mlp_embedding_item')\n",
    "            mlp_embed_item = tf.nn.embedding_lookup(mlp_embedding_item, self.item_inp)\n",
    "            \n",
    "            # MF part\n",
    "            mf_user_latent = tf.reshape(mf_embed_user, [-1, mf_embed_user.shape[-1]])\n",
    "            mf_item_latent = tf.reshape(mf_embed_item, [-1, mf_embed_item.shape[-1]])\n",
    "            mf_vector = tf.multiply(mf_user_latent, mf_item_latent)\n",
    "            # MLP part\n",
    "            mlp_user_latent = tf.reshape(mlp_embed_user, [-1, mlp_embed_user.shape[-1]])\n",
    "            mlp_item_latent = tf.reshape(mlp_embed_item, [-1, mlp_embed_item.shape[-1]])\n",
    "            mlp_vector = tf.concat([mlp_user_latent, mlp_item_latent], axis=-1)\n",
    "            num_layer = len(layers)\n",
    "            for idx in range(1, num_layer):\n",
    "                mlp_vector = tf.layers.dense(mlp_vector, layers[idx], activation=tf.nn.relu, name=\"layer{}\".format(idx))\n",
    "\n",
    "            self.predict_vector = tf.concat([mf_vector, mlp_vector], axis =-1)\n",
    "            \n",
    "            prediction = tf.layers.dense(inputs=self.predict_vector,\n",
    "                                              kernel_initializer = pred_init,\n",
    "                                              bias_initializer = pred_init,\n",
    "                                              units=1, activation='sigmoid')\n",
    "            self.prediction = tf.reshape(prediction, [-1], name='prediction')\n",
    "            self.loss = tf.keras.losses.binary_crossentropy(y_pred=self.prediction, y_true=self.y_true)\n",
    "#             self.loss = tf.reduce_mean((self.y_true*tf.log(self.prediction)) + ((1-self.y_true)*tf.log(1-self.prediction))) # (Y_TRUE - PREDICTION)\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "            self.train_op = self.optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(n_fake_users, n_users, n_movies):\n",
    "    num_users = n_fake_users + n_users\n",
    "    num_items = n_movies\n",
    "    batch_size = 512\n",
    "    \n",
    "    #model params:\n",
    "    mf_dim = 8\n",
    "    layers = [64, 32, 16, 8]\n",
    "    reg_layers = [0, 0, 0, 0]\n",
    "    reg_mf = 0\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    model = Model(num_users = num_users,\n",
    "                  num_items = num_items,\n",
    "                  mf_dim = mf_dim,\n",
    "                  layers= layers,\n",
    "                  reg_layers=reg_layers,\n",
    "                  reg_mf=reg_mf)\n",
    "    print(f'model with: n_fake_users={n_fake_users}, n_users={n_users}, n_movies={n_movies} created')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFPredictWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    def predict(self, user_item, verbose):\n",
    "        [u, i] = user_item\n",
    "        model = self.model\n",
    "        preds = sess.run(model.prediction,\n",
    "                          feed_dict={model.user_inp: u,\n",
    "                           model.item_inp: i})\n",
    "                                            \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "N_FAKE_USERS = 8\n",
    "# https://stackoverflow.com/questions/49842705/reuse-tensorflow-session-without-making-a-checkpoint\n",
    "\n",
    "def get_batches(training_set, batch_size ):\n",
    "    training_mat = np.column_stack(training_set)\n",
    "    np.random.shuffle(training_mat)\n",
    "    # return np.vsplit(training_mat, training_mat.shape[0] // batch_size)\n",
    "    batches = np.array_split(training_mat, training_mat.shape[0] // batch_size)\n",
    "    return batches\n",
    "    # print(len(batches))\n",
    "epochs = 10\n",
    "batch_size = 512\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    model = get_model(n_fake_users=N_FAKE_USERS, n_users=n_users, n_movies=n_movies)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    model_p = TFPredictWrapper(model)\n",
    "    # training and validation\n",
    "    for e in range(epochs):\n",
    "        batches = get_batches(train_set, batch_size = batch_size)\n",
    "        loses_in_epoch = []\n",
    "        for b in batches:\n",
    "            u = b[:,0]\n",
    "            i = b[:,1]\n",
    "            r = b[:,2]\n",
    "            _, preds, loss = sess.run([model.train_op, model.prediction, model.loss],\n",
    "                                      feed_dict={model.user_inp: u,\n",
    "                                       model.item_inp: i,\n",
    "                                       model.y_true: r})\n",
    "            loses_in_epoch.append(np.mean(loss))\n",
    "        epoch_loss = np.mean(loses_in_epoch)\n",
    "\n",
    "        mean_hr, mean_ndcg, time_eval = evaluate_model(model_p, test_set, verbose=0)\n",
    "        print('e={} hr={:.3f} ndcg={:.3f} time={:.3f} train_loss={:.3f} '.format(e+1, mean_hr, mean_ndcg, time_eval, epoch_loss))\n",
    "    save_path = saver.save(sess, \"model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    model = get_model(n_fake_users=N_FAKE_USERS, n_users=n_users, n_movies=n_movies)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, \"model.ckpt\")\n",
    "    model_p = TFPredictWrapper(model)\n",
    "    mean_hr, mean_ndcg, time_eval = evaluate_model(model_p, test_set, verbose=0)\n",
    "    print('e={} hr={:.3f} ndcg={:.3f} time={:.3f}'.format(e+1, mean_hr, mean_ndcg, time_eval))\n",
    "#     preds = model.sess.run(model.prediction,\n",
    "#                               feed_dict={model.user_inp: users,\n",
    "#                                model.item_inp: items})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mal_train_set = [users + train_set[0], items + train_set[1]]\n",
    "output_dim = (N_FAKE_USERS-1) * (n_movies-1)\n",
    "fake_ratings = np.full((output_dim,), 2)\n",
    "cartesian_prodcut = np.array([[n_users + user - 1, item] \n",
    "                              for user in np.arange(1, N_FAKE_USERS) for item in np.arange(1, n_movies)])\n",
    "attack_users = cartesian_prodcut[:, 0]\n",
    "attack_items = cartesian_prodcut[:, 1]\n",
    "# np.append(fake_ratings, train_set[2])\n",
    "attack_train_set = np.array([np.append(attack_users, train_set[0]),\n",
    "                          np.append(attack_items, train_set[1])])\n",
    "real_training_rating_size = len(train_set[2])\n",
    "print('attack rating size=', output_dim)\n",
    "print('real_training_rating_size=', real_training_rating_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_dim = (N_FAKE_USERS-1) * (n_movies-1)\n",
    "tf.reset_default_graph()\n",
    "mal_real_shape = output_dim + real_training_rating_size\n",
    "rating_input = tf.placeholder(tf.float32, [mal_real_shape], name='benzona')\n",
    "model = get_model(n_fake_users=N_FAKE_USERS, n_users=n_users, n_movies=n_movies)\n",
    "print(rating_input.shape)\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, \"model.ckpt\")\n",
    "    eps_p = tf.placeholder(tf.float32, mal_real_shape)\n",
    "    mask_p = tf.placeholder(tf.float32, mal_real_shape)\n",
    "#     prediction = tf.layers.dense(inputs=model.predict_vector,units=1, activation='sigmoid')\n",
    "#     prediction = tf.reshape(prediction, [-1], name='prediction')\n",
    "\n",
    "    # look for an option to limit amount of poison\n",
    "    theta = 1\n",
    "    # could use a hinge loss - mean below average will not tolerate.\n",
    "    # reg might need to be negative, loss itself positive and increasing\n",
    "    # rint will round those under 0.5, while higher will count as loss.\n",
    "#     reg_loss = -theta*tf.reduce_mean(rating_input*mask_p) # theta=3.5\n",
    "    reg_loss = -theta*(tf.reduce_sum(tf.rint(rating_input * mask_p)) / N_FAKE_USERS)\n",
    "    obj_loss = tf.keras.losses.binary_crossentropy(y_pred=model.prediction, y_true=rating_input)\n",
    "    combined_loss = reg_loss + obj_loss\n",
    "    grad = tf.gradients(combined_loss, rating_input) # take the gradient of the loss according to prediction # check\n",
    "    # new_labels_op = tf.stop_gradient(rating_input + epsilon * tf.sign(grad)) # perform a gradient step\n",
    "#     rating_input_out = tf.stop_gradient(rating_input + (eps_p * tf.sign(grad) * mask_p)) # perform a gradient step\n",
    "    rating_input_out = tf.stop_gradient(rating_input + (eps_p * grad * mask_p)) # perform a gradient step\n",
    "    rating_input_out = tf.reshape(tf.clip_by_value(rating_input_out, 0, 1), (-1,))\n",
    "    step = 1000\n",
    "    eps = np.full((mal_real_shape,), step)\n",
    "#     y_true = np.random.randint(0, 2, (output_dim,)) ### WHATT IS THIS?\n",
    "    # try to emulate a session with all data - training_set + adver, then the gradient will adjust adver rating params\n",
    "#     new_labels = np.full((output_dim,), 0.01) # initiate new ratings at 0.5\n",
    "    new_labels_mal_part = np.full((output_dim,), 0.5)\n",
    "    new_labels = np.concatenate([new_labels_mal_part, train_set[2]])\n",
    "    all_users = mal_train_set[0]\n",
    "    all_items = mal_train_set[1]\n",
    "    indexes = np.arange(len(all_users))\n",
    "#new_labels.shape\n",
    "    for i in range(50):\n",
    "        ## Todo: There is an issue in creating a shuffle while keeping track in the order of the mal data\n",
    "#         print(i)\n",
    "#         p = np.random.permutation(len(new_labels))\n",
    "#         all_users = all_users[p]\n",
    "#         all_items = all_items[p]\n",
    "#         new_labels = new_labels[p]\n",
    "#         new_labels_before_change = new_labels.copy()\n",
    "        mask = np.zeros((mal_real_shape,))\n",
    "#         indexes = indexes[p] # this will help get the mal ratings in order as we started\n",
    "        mask[np.argwhere(indexes < output_dim)] = 1\n",
    "        \n",
    "        new_labels, adv_l, obj_l, reg_l= sess.run([rating_input_out, combined_loss, obj_loss, reg_loss],\n",
    "                                 feed_dict={model.user_inp: all_users,\n",
    "                                            model.item_inp: all_items,\n",
    "                                            rating_input: new_labels,\n",
    "                                            mask_p: mask,\n",
    "                                            eps_p: eps})\n",
    "        new_labels = new_labels.reshape((-1,))\n",
    "#         new_labels[np.argwhere(indexes >= output_dim)] = new_labels_before_change[np.argwhere(indexes >= output_dim)]\n",
    "        attack_rating = new_labels[np.argwhere(indexes < output_dim)][np.arange(output_dim)].reshape(-1,)\n",
    "        ## Safety Check such that legit ratings did not change\n",
    "        legit_rating = new_labels[np.argwhere(indexes > output_dim)].reshape(-1,)\n",
    "        assert len(np.unique(legit_rating)) ==2, 'legit rating must not change during gradient step'\n",
    "        print(i, np.round(attack_rating[:10], 2), np.mean(attack_rating), adv_l, obj_l, reg_l)\n",
    "#         print('adv_l={:0.4f}, max={:.4f} min={:.4f} count={}'.format(adv_l, np.max(mal_only), np.min(mal_only), len(np.unique(mal_only))))\n",
    "#         new_labels_before_change[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for each user, we want to keep all positive ones, and keep excatly positive_ones* 4 zeros.\n",
    "assert len(attack_users) == len(attack_items)\n",
    "assert len(attack_users) == len(attack_rating)\n",
    "import pandas as pd\n",
    "base_attack_df = pd.DataFrame({'u': attack_users, 'i':attack_items, 'r': attack_rating})\n",
    "# maybe its better to take top ## instead of rint\n",
    "base_attack_df.r.hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[:].groupby('u').apply(lambda x: print(x))\n",
    "treshold = 0.5\n",
    "attack_df_filterd = base_attack_df.copy()\n",
    "# attack_df_filterd['r'] = attack_df_filterd['r'].apply(lambda x: np.rint(x))\n",
    "attack_df_filterd = base_attack_df[base_attack_df.apply(lambda x: x.r > treshold, axis=1)] # apply on rows, keep only high prob ratings\n",
    "attack_df_filterd['r'] = attack_df_filterd['r'].apply(lambda x: 1) #|.astype(int)\n",
    "attack_df_filterd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add negative sampling to the mal data frame\n",
    "from tqdm import tqdm\n",
    "df = attack_df_filterd\n",
    "negative_items = {}\n",
    "users = list(sorted(df['u'].unique()))\n",
    "for idx, user in tqdm(enumerate(users), total=len(users)):\n",
    "    negative_items[user] = df[df['u'] != user]['i'].unique()\n",
    "negative_items\n",
    "num_negatives = 4\n",
    "df = df.groupby('u').apply(lambda s: s.sample(frac=1))\n",
    "user_input, item_input, labels = [], [], []\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    user = row['u']\n",
    "    user_input.append(user)\n",
    "    item_input.append(row['i'])\n",
    "    labels.append(row['r'])\n",
    "    negative_input_items = np.random.choice(negative_items[user], num_negatives)\n",
    "    for neg_item in negative_input_items:\n",
    "        user_input.append(user)\n",
    "        item_input.append(neg_item)\n",
    "        labels.append(0)\n",
    "\n",
    "mal_training_set = (np.array(user_input), np.array(item_input), np.array(labels))\n",
    "# mal_training_set = (df['u'].values, df['i'].values, df['r'].values)\n",
    "mal_training_set # this already has the inputs and negative items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data import create_subset, concat_and_shuffle\n",
    "TRAIN_FRAC = 0.10\n",
    "malicious_training_set = mal_training_set\n",
    "# malicious_training_set = (df_filterd.values[:,0], df_filterd.values[:,1], df_filterd.values[:,2])\n",
    "print('df_filterd', len(df_filterd))\n",
    "print('train_set',len(train_set[0]))\n",
    "train_set_subset = create_subset(train_set, train_frac=TRAIN_FRAC)\n",
    "attack_benign_training_set = concat_and_shuffle(malicious_training_set, train_set_subset)\n",
    "print(len(malicious_training_set[0]))\n",
    "len(attack_benign_training_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model using the fake predictions\n",
    "tf.reset_default_graph()\n",
    "model = get_model(n_fake_users=N_FAKE_USERS, n_users=n_users, n_movies=n_movies)\n",
    "model_p = TFPredictWrapper(model)\n",
    "mal_epochs = 5\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, \"model.ckpt\")\n",
    "    for e in range(mal_epochs):\n",
    "        batches = get_batches(attack_benign_training_set, batch_size = batch_size)\n",
    "        u = attack_benign_training_set[0]\n",
    "        i = attack_benign_training_set[1]\n",
    "        r = attack_benign_training_set[2]\n",
    "        _, preds, loss = sess.run([model.train_op, model.prediction, model.loss],\n",
    "                                  feed_dict={model.user_inp: u,\n",
    "                                   model.item_inp: i,\n",
    "                                   model.y_true: r})\n",
    "            \n",
    "        epoch_loss = np.mean(loses_in_epoch)\n",
    "        mean_hr, mean_ndcg, time_eval = evaluate_model(model_p, test_set, verbose=0)\n",
    "        print('{:.3f} {:.3f} {:.3f} {:.3f}'.format(epoch_loss, mean_hr, mean_ndcg, time_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model using the fake predictions\n",
    "tf.reset_default_graph()\n",
    "model = get_model(n_fake_users=N_FAKE_USERS, n_users=n_users, n_movies=n_movies)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, \"model.ckpt\")\n",
    "    model_p = TFPredictWrapper(model)\n",
    "    for e in range(mal_epochs):\n",
    "        batches = get_batches(attack_benign_training_set, batch_size = batch_size)\n",
    "        loses_in_epoch = []\n",
    "        for b in batches:\n",
    "            u = b[:,0]\n",
    "            i = b[:,1]\n",
    "            r = b[:,2]\n",
    "            _, preds, loss = sess.run([model.train_op, model.prediction, model.loss],\n",
    "                                      feed_dict={model.user_inp: u,\n",
    "                                       model.item_inp: i,\n",
    "                                       model.y_true: r})\n",
    "            loses_in_epoch.append(np.mean(loss))\n",
    "        epoch_loss = np.mean(loses_in_epoch)\n",
    "        mean_hr, mean_ndcg, time_eval = evaluate_model(model_p, test_set, verbose=0)\n",
    "        print('{:.3f} {:.3f} {:.3f} {:.3f}'.format(epoch_loss, mean_hr, mean_ndcg, time_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with model.sess:\n",
    "new_inp.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(1, n_fake_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"FGSM ATTACK\"\"\"\n",
    "\n",
    "# user_inp = tf.placeholder(tf.float32, (n_fake_users, 64))\n",
    "# item_inp = tf.placeholder(tf.float32, (n_movies, 64))\n",
    "# # true_out = tf.placeholder(tf.float32, (n_fake_users, 10))\n",
    "epsilon = tf.placeholder(tf.float32, [n_fake_users * n_movies])\n",
    "# user_inp = tf.Variable((n_fake_users, 64), tf.float32)\n",
    "# item_inp = tf.Variable((n_fake_users, 64), tf.float32)\n",
    "# # true_out = tf.placeholder(tf.float32, (n_fake_users, 10))\n",
    "# epsilon = tf.Variable([n_fake_users, 64], tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    mean_hr, mean_ndcg, time_eval = evaluate_model(model_p, test_set, verbose=0)\n",
    "    preds = sess.run(model.prediction,\n",
    "                              feed_dict={model.user_inp: users,\n",
    "                               model.item_inp: items})\n",
    "    grad = tf.gradients(model.loss, [self.rating_input])\n",
    "# grad\n",
    "# grad1 = tf.gradients(loss, new_inp)\n",
    "# grad = tf.where(tf.is_nan(grad1), tf.zeros_like(grad1)*1, grad1) #replace nans with zeros\n",
    "# new_inp = tf.stop_gradient(data + epsilon * tf.sign(grad))\n",
    "# new_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"FGSM ATTACK\"\"\"\n",
    "\n",
    "n_adversarial = y_test.shape[0]\n",
    "\n",
    "trgt_inp = tf.placeholder(tf.float32, (n_fake_users, 64))\n",
    "# true_out = tf.placeholder(tf.float32, (n_fake_users, 10))\n",
    "epsilon = tf.placeholder(tf.float32, [n_adversarial, img_width, img_height, 1])\n",
    "\n",
    "new_inp = tf.identity(trgt_inp)  #returns a tensor with the same shape and type\n",
    "\n",
    "output = model(new_inp)\n",
    "\n",
    "loss = tf.sqrt(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=true_out))\n",
    "\n",
    "grad = tf.gradients(loss, new_inp)\n",
    "new_inp = tf.stop_gradient(new_inp + epsilon * tf.sign(grad))\n",
    "\n",
    "step = 0.1\n",
    "eps = np.full((n_adversarial, img_width, img_height, 1), step)\n",
    "\n",
    "feed_dict = {trgt_inp: x_test,\n",
    "             true_out: y_test,\n",
    "             epsilon: eps}\n",
    "\n",
    "x_adv, l = sess.run([new_inp, loss],\n",
    "                    feed_dict)\n",
    "\n",
    "x_adv = x_adv.reshape((n_adversarial, img_width, img_height, 1))\n",
    "\n",
    "acc_bfr_attck = model.evaluate(x=x_test, y=y_test, batch_size=32)[1]\n",
    "acc_aftr_attck = model.evaluate(x=x_adv, y=y_test, batch_size=32)[1]\n",
    "print('Accuracy before FGSM {}, Accuracy after FGSM {}'.format(acc_bfr_attck, acc_aftr_attck))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[2.]]\n",
    "m = tf.matmul(x, x)\n",
    "print(\"hello, {}\".format(m))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.zeros((10,))\n",
    "indexes = [1, 3, 5 ,7]\n",
    "t[indexes] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[indexes] = not t[indexes].all\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.000000000009\n",
    "-(1*np.log(p) + 0 * np.log(1-p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path - Running mode: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lidora/opt/anaconda3/envs/ars/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/lidora/opt/anaconda3/envs/ars/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/lidora/opt/anaconda3/envs/ars/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/lidora/opt/anaconda3/envs/ars/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/lidora/opt/anaconda3/envs/ars/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/lidora/opt/anaconda3/envs/ars/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/lidora/opt/anaconda3/envs/ars/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/lidora/opt/anaconda3/envs/ars/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/lidora/opt/anaconda3/envs/ars/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/lidora/opt/anaconda3/envs/ars/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/lidora/opt/anaconda3/envs/ars/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/lidora/opt/anaconda3/envs/ars/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/lidora/opt/anaconda3/envs/ars/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "from NeuMF import load_base_model\n",
    "import Constants\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "#943 n_movies: 1682\n",
    "DATASET_NAME = 'movielens100k'\n",
    "model = load_base_model(16, DATASET_NAME, Constants.BASE_MODEL_DIR, Constants.BASE_MODEL_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'prediction/Sigmoid:0' shape=(?, 1) dtype=float32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_inp_p = model.inputs[0]\n",
    "item_inp_p = model.inputs[1]\n",
    "out_rating_p = model.outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.44884193],\n",
      "       [0.44884193]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "session = keras.backend.get_session()\n",
    "with session as sess:\n",
    "    import numpy as np\n",
    "    ratings = sess.run([out_rating_p], {user_inp_p:np.array([0,0]).reshape(-1,1),\n",
    "                                  item_inp_p: np.array([0,0]).reshape(-1, 1)})\n",
    "    print(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
