{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataLoader import *\n",
    "from Data import Data\n",
    "from Evalute import evaluate_model\n",
    "# from Models.SimpleCF import SimpleCF\n",
    "# from NeuMF import get_model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gast==0.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started...\n",
      "Dataset: movielens100k , convert_binary: True\n",
      "pre_processing.. \n",
      "n_real_users: 943 n_movies: 1682\n",
      "train_set size (/w negative sampling): 495285 test_set: size 943\n"
     ]
    }
   ],
   "source": [
    "convert_binary = True\n",
    "load_model = False\n",
    "testset_percentage = 0.2\n",
    "\n",
    "\n",
    "print('Started...')\n",
    "DATASET_NAME = 'movielens100k'\n",
    "# DATASET_NAME = 'movielens1m'\n",
    "df = get_from_dataset_name(DATASET_NAME, True)\n",
    "data = Data(seed=42)\n",
    "train_set, test_set, n_users, n_movies = data.pre_processing(df, test_percent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_fake_users = 300\n",
    "# # possible the need to have n_movies+1\n",
    "# fake_users_mat = np.zeros((n_fake_users , n_movies))\n",
    "# n_users_w_fake = n_fake_users + n_users + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras\n",
    "# from keras.layers import Embedding, Input, Flatten\n",
    "import tensorflow as tf\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, num_users, num_items, mf_dim=8, layers=[64, 32, 16, 8], reg_layers=[0, 0, 0, 0], reg_mf=0):\n",
    "        with tf.variable_scope('NeuCF_network'):\n",
    "            self.user_inp = tf.placeholder(dtype=tf.int32, shape=[None], name='user_input')\n",
    "            self.item_inp = tf.placeholder(dtype=tf.int32, shape=[None], name='item_input')\n",
    "            self.y_true = tf.placeholder(dtype=tf.float32, shape=[None], name='y_true')\n",
    "\n",
    "            normal_init = tf.random_normal_initializer(mean=0, stddev=0.01)\n",
    "            pred_init = tf.keras.initializers.lecun_uniform(seed=None)\n",
    "            # TODO: Add regulaizer\n",
    "            \n",
    "            mf_embedding_user = tf.Variable(normal_init([num_users, mf_dim]), name='mf_embedding_user')\n",
    "            mf_embed_user = tf.nn.embedding_lookup(mf_embedding_user, self.user_inp)\n",
    "\n",
    "            mf_embedding_item = tf.Variable(normal_init([num_items, mf_dim]), name='mf_embedding_item')\n",
    "            mf_embed_item = tf.nn.embedding_lookup(mf_embedding_item, self.item_inp)\n",
    "\n",
    "            mlp_embedding_user = tf.Variable(normal_init([num_users, int(layers[0] / 2)]), name='mlp_embedding_user')\n",
    "            mlp_embed_user = tf.nn.embedding_lookup(mlp_embedding_user, self.user_inp)\n",
    "\n",
    "            mlp_embedding_item = tf.Variable(normal_init([num_items, int(layers[0] / 2)]), name='mlp_embedding_item')\n",
    "            mlp_embed_item = tf.nn.embedding_lookup(mlp_embedding_item, self.item_inp)\n",
    "            \n",
    "            # MF part\n",
    "            mf_user_latent = tf.reshape(mf_embed_user, [-1, mf_embed_user.shape[-1]])\n",
    "            mf_item_latent = tf.reshape(mf_embed_item, [-1, mf_embed_item.shape[-1]])\n",
    "            mf_vector = tf.multiply(mf_user_latent, mf_item_latent)\n",
    "            # MLP part\n",
    "            mlp_user_latent = tf.reshape(mlp_embed_user, [-1, mlp_embed_user.shape[-1]])\n",
    "            mlp_item_latent = tf.reshape(mlp_embed_item, [-1, mlp_embed_item.shape[-1]])\n",
    "            mlp_vector = tf.concat([mlp_user_latent, mlp_item_latent], axis=-1)\n",
    "            num_layer = len(layers)\n",
    "            for idx in range(1, num_layer):\n",
    "                mlp_vector = tf.layers.dense(mlp_vector, layers[idx], activation=tf.nn.relu, name=\"layer{}\".format(idx))\n",
    "\n",
    "            self.predict_vector = tf.concat([mf_vector, mlp_vector], axis =-1)\n",
    "            \n",
    "            prediction = tf.layers.dense(inputs=self.predict_vector,\n",
    "                                              kernel_initializer = pred_init,\n",
    "                                              bias_initializer = pred_init,\n",
    "                                              units=1, activation='sigmoid')\n",
    "            self.prediction = tf.reshape(prediction, [-1], name='prediction')\n",
    "            self.loss = tf.keras.losses.binary_crossentropy(y_pred=self.prediction, y_true=self.y_true)\n",
    "#             self.loss = tf.reduce_mean((self.y_true*tf.log(self.prediction)) + ((1-self.y_true)*tf.log(1-self.prediction))) # (Y_TRUE - PREDICTION)\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "            self.train_op = self.optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(n_fake_users, n_users, n_movies):\n",
    "    num_users = n_fake_users + n_users\n",
    "    num_items = n_movies\n",
    "    batch_size = 512\n",
    "    \n",
    "    #model params:\n",
    "    mf_dim = 8\n",
    "    layers = [64, 32, 16, 8]\n",
    "    reg_layers = [0, 0, 0, 0]\n",
    "    reg_mf = 0\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    model = Model(num_users = num_users,\n",
    "                  num_items = num_items,\n",
    "                  mf_dim = mf_dim,\n",
    "                  layers= layers,\n",
    "                  reg_layers=reg_layers,\n",
    "                  reg_mf=reg_mf)\n",
    "    print(f'model with: n_fake_users={n_fake_users}, n_users={n_users}, n_movies={n_movies} created')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFPredictWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    def predict(self, user_item, verbose):\n",
    "        [u, i] = user_item\n",
    "        model = self.model\n",
    "        preds = sess.run(model.prediction,\n",
    "                          feed_dict={model.user_inp: u,\n",
    "                           model.item_inp: i})\n",
    "                                            \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model with: n_fake_users=8, n_users=943, n_movies=1682 created\n",
      "e=1 hr=0.446 ndcg=0.247 time=0.285 train_loss=0.424 \n",
      "e=2 hr=0.511 ndcg=0.269 time=0.262 train_loss=0.362 \n",
      "e=3 hr=0.528 ndcg=0.282 time=0.278 train_loss=0.342 \n",
      "e=4 hr=0.522 ndcg=0.280 time=0.253 train_loss=0.332 \n",
      "e=5 hr=0.531 ndcg=0.281 time=0.244 train_loss=0.326 \n",
      "e=6 hr=0.541 ndcg=0.289 time=0.248 train_loss=0.320 \n",
      "e=7 hr=0.537 ndcg=0.283 time=0.263 train_loss=0.316 \n",
      "e=8 hr=0.542 ndcg=0.285 time=0.250 train_loss=0.311 \n",
      "e=9 hr=0.537 ndcg=0.280 time=0.327 train_loss=0.307 \n",
      "e=10 hr=0.530 ndcg=0.280 time=0.285 train_loss=0.303 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "N_FAKE_USERS = 8\n",
    "# https://stackoverflow.com/questions/49842705/reuse-tensorflow-session-without-making-a-checkpoint\n",
    "\n",
    "def get_batches(training_set, batch_size ):\n",
    "    training_mat = np.column_stack(training_set)\n",
    "    np.random.shuffle(training_mat)\n",
    "    # return np.vsplit(training_mat, training_mat.shape[0] // batch_size)\n",
    "    batches = np.array_split(training_mat, training_mat.shape[0] // batch_size)\n",
    "    return batches\n",
    "    # print(len(batches))\n",
    "epochs = 10\n",
    "batch_size = 512\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    model = get_model(n_fake_users=N_FAKE_USERS, n_users=n_users, n_movies=n_movies)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    model_p = TFPredictWrapper(model)\n",
    "    # training and validation\n",
    "    for e in range(epochs):\n",
    "        batches = get_batches(train_set, batch_size = batch_size)\n",
    "        loses_in_epoch = []\n",
    "        for b in batches:\n",
    "            u = b[:,0]\n",
    "            i = b[:,1]\n",
    "            r = b[:,2]\n",
    "            _, preds, loss = sess.run([model.train_op, model.prediction, model.loss],\n",
    "                                      feed_dict={model.user_inp: u,\n",
    "                                       model.item_inp: i,\n",
    "                                       model.y_true: r})\n",
    "            loses_in_epoch.append(np.mean(loss))\n",
    "        epoch_loss = np.mean(loses_in_epoch)\n",
    "\n",
    "        mean_hr, mean_ndcg, time_eval = evaluate_model(model_p, test_set, verbose=0)\n",
    "        print('e={} hr={:.3f} ndcg={:.3f} time={:.3f} train_loss={:.3f} '.format(e+1, mean_hr, mean_ndcg, time_eval, epoch_loss))\n",
    "    save_path = saver.save(sess, \"model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model with: n_fake_users=8, n_users=943, n_movies=1682 created\n",
      "WARNING:tensorflow:From /Users/lidora/opt/anaconda3/envs/ars/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from model.ckpt\n",
      "e=10 hr=0.530 ndcg=0.280 time=0.271\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    model = get_model(n_fake_users=N_FAKE_USERS, n_users=n_users, n_movies=n_movies)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, \"model.ckpt\")\n",
    "    model_p = TFPredictWrapper(model)\n",
    "    mean_hr, mean_ndcg, time_eval = evaluate_model(model_p, test_set, verbose=0)\n",
    "    print('e={} hr={:.3f} ndcg={:.3f} time={:.3f}'.format(e+1, mean_hr, mean_ndcg, time_eval))\n",
    "#     preds = model.sess.run(model.prediction,\n",
    "#                               feed_dict={model.user_inp: users,\n",
    "#                                model.item_inp: items})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack rating size= 11767\n",
      "real_training_rating_size= 495285\n"
     ]
    }
   ],
   "source": [
    "# mal_train_set = [users + train_set[0], items + train_set[1]]\n",
    "output_dim = (N_FAKE_USERS-1) * (n_movies-1)\n",
    "fake_ratings = np.full((output_dim,), 2)\n",
    "cartesian_prodcut = np.array([[n_users + user - 1, item] \n",
    "                              for user in np.arange(1, N_FAKE_USERS) for item in np.arange(1, n_movies)])\n",
    "attack_users = cartesian_prodcut[:, 0]\n",
    "attack_items = cartesian_prodcut[:, 1]\n",
    "# np.append(fake_ratings, train_set[2])\n",
    "attack_train_set = np.array([np.append(attack_users, train_set[0]),\n",
    "                          np.append(attack_items, train_set[1])])\n",
    "real_training_rating_size = len(train_set[2])\n",
    "print('attack rating size=', output_dim)\n",
    "print('real_training_rating_size=', real_training_rating_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model with: n_fake_users=8, n_users=943, n_movies=1682 created\n",
      "(507052,)\n",
      "INFO:tensorflow:Restoring parameters from model.ckpt\n",
      "0 [0.5  0.51 0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5 ] 0.634248\n",
      "1 [0.5  0.52 0.51 0.5  0.5  0.5  0.51 0.51 0.5  0.5 ] 0.634248\n",
      "2 [0.5  0.53 0.51 0.51 0.51 0.5  0.51 0.51 0.5  0.5 ] 0.634248\n",
      "3 [0.5  0.54 0.52 0.51 0.51 0.5  0.52 0.51 0.5  0.5 ] 0.634248\n",
      "4 [0.5  0.55 0.52 0.51 0.51 0.5  0.52 0.51 0.51 0.5 ] 0.634248\n",
      "5 [0.5  0.56 0.53 0.51 0.51 0.5  0.53 0.52 0.51 0.5 ] 0.634248\n",
      "6 [0.5  0.57 0.53 0.52 0.51 0.5  0.53 0.52 0.51 0.5 ] 0.634248\n",
      "7 [0.5  0.58 0.54 0.52 0.52 0.5  0.54 0.52 0.51 0.5 ] 0.634248\n",
      "8 [0.5  0.59 0.54 0.52 0.52 0.5  0.54 0.52 0.51 0.5 ] 0.634248\n",
      "9 [0.5  0.6  0.55 0.52 0.52 0.5  0.54 0.53 0.51 0.5 ] 0.634248\n",
      "10 [0.5  0.61 0.55 0.53 0.52 0.5  0.55 0.53 0.51 0.5 ] 0.634248\n",
      "11 [0.5  0.62 0.55 0.53 0.52 0.5  0.55 0.53 0.51 0.5 ] 0.634248\n",
      "12 [0.5  0.63 0.56 0.53 0.52 0.5  0.56 0.53 0.51 0.5 ] 0.634248\n",
      "13 [0.5  0.64 0.56 0.53 0.53 0.5  0.56 0.54 0.52 0.5 ] 0.634248\n",
      "14 [0.5  0.65 0.57 0.53 0.53 0.5  0.57 0.54 0.52 0.51] 0.634248\n",
      "15 [0.5  0.66 0.57 0.54 0.53 0.5  0.57 0.54 0.52 0.51] 0.634248\n",
      "16 [0.5  0.67 0.58 0.54 0.53 0.5  0.58 0.54 0.52 0.51] 0.634248\n",
      "17 [0.5  0.68 0.58 0.54 0.53 0.5  0.58 0.55 0.52 0.51] 0.634248\n",
      "18 [0.51 0.69 0.59 0.54 0.54 0.5  0.59 0.55 0.52 0.51] 0.634248\n",
      "19 [0.51 0.7  0.59 0.55 0.54 0.5  0.59 0.55 0.52 0.51] 0.634248\n"
     ]
    }
   ],
   "source": [
    "output_dim = (N_FAKE_USERS-1) * (n_movies-1)\n",
    "tf.reset_default_graph()\n",
    "mal_real_shape = output_dim + real_training_rating_size\n",
    "rating_input = tf.placeholder(tf.float32, [mal_real_shape], name='benzona')\n",
    "model = get_model(n_fake_users=N_FAKE_USERS, n_users=n_users, n_movies=n_movies)\n",
    "print(rating_input.shape)\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, \"model.ckpt\")\n",
    "    eps_p = tf.placeholder(tf.float32, mal_real_shape)\n",
    "    mask_p = tf.placeholder(tf.float32, mal_real_shape)\n",
    "#     prediction = tf.layers.dense(inputs=model.predict_vector,units=1, activation='sigmoid')\n",
    "#     prediction = tf.reshape(prediction, [-1], name='prediction')\n",
    "    new_loss = tf.keras.losses.binary_crossentropy(y_pred=model.prediction, y_true=rating_input)\n",
    "    grad = tf.gradients(new_loss, rating_input) # take the gradient of the loss according to prediction # check\n",
    "    # new_labels_op = tf.stop_gradient(rating_input + epsilon * tf.sign(grad)) # perform a gradient step\n",
    "#     rating_input_out = tf.stop_gradient(rating_input + (eps_p * tf.sign(grad) * mask_p)) # perform a gradient step\n",
    "    rating_input_out = tf.stop_gradient(rating_input + (eps_p * grad * mask_p)) # perform a gradient step\n",
    "    rating_input_out = tf.reshape(tf.clip_by_value(rating_input_out, 0, 1), (-1,))\n",
    "    step = 1000\n",
    "    eps = np.full((mal_real_shape,), step)\n",
    "#     y_true = np.random.randint(0, 2, (output_dim,)) ### WHATT IS THIS?\n",
    "    # try to emulate a session with all data - training_set + adver, then the gradient will adjust adver rating params\n",
    "#     new_labels = np.full((output_dim,), 0.01) # initiate new ratings at 0.5\n",
    "    new_labels_mal_part = np.full((output_dim,), 0.5)\n",
    "    new_labels = np.concatenate([new_labels_mal_part, train_set[2]])\n",
    "    all_users = mal_train_set[0]\n",
    "    all_items = mal_train_set[1]\n",
    "    indexes = np.arange(len(all_users))\n",
    "#new_labels.shape\n",
    "    for i in range(20):\n",
    "        ## Todo: There is an issue in creating a shuffle while keeping track in the order of the mal data\n",
    "#         print(i)\n",
    "#         p = np.random.permutation(len(new_labels))\n",
    "#         all_users = all_users[p]\n",
    "#         all_items = all_items[p]\n",
    "#         new_labels = new_labels[p]\n",
    "#         new_labels_before_change = new_labels.copy()\n",
    "        mask = np.zeros((mal_real_shape,))\n",
    "#         indexes = indexes[p] # this will help get the mal ratings in order as we started\n",
    "        mask[np.argwhere(indexes < output_dim)] = 1\n",
    "        \n",
    "        new_labels, adv_l= sess.run([rating_input_out, new_loss],\n",
    "                                 feed_dict={model.user_inp: all_users,\n",
    "                                            model.item_inp: all_items,\n",
    "                                            rating_input: new_labels,\n",
    "                                            mask_p: mask,\n",
    "                                            eps_p: eps})\n",
    "        new_labels = new_labels.reshape((-1,))\n",
    "#         new_labels[np.argwhere(indexes >= output_dim)] = new_labels_before_change[np.argwhere(indexes >= output_dim)]\n",
    "        attack_rating = new_labels[np.argwhere(indexes < output_dim)][np.arange(output_dim)].reshape(-1,)\n",
    "        ## Safety Check such that legit ratings did not change\n",
    "        legit_rating = new_labels[np.argwhere(indexes > output_dim)].reshape(-1,)\n",
    "        assert len(np.unique(legit_rating)) ==2, 'legit rating must not change during gradient step'\n",
    "        print(i, np.round(attack_rating[:10], 2), np.mean(mal_only))\n",
    "#         print('adv_l={:0.4f}, max={:.4f} min={:.4f} count={}'.format(adv_l, np.max(mal_only), np.min(mal_only), len(np.unique(mal_only))))\n",
    "#         new_labels_before_change[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5052762, 0.7008748, 0.5914347, ..., 0.       , 0.       ,\n",
       "       0.       ], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(attack_users) == len(attack_items)\n",
    "assert len(attack_users) == len(attack_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x13931ee48>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for each user, we want to keep all positive ones, and keep excatly positive_ones* 4 zeros.\n",
    "import pandas as pd\n",
    "base_attack_df = pd.DataFrame({'u': attack_users, 'i':attack_items, 'r_per': attack_rating})\n",
    "# maybe its better to take top ## instead of rint\n",
    "base_attack_df.r.hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lidora/opt/anaconda3/envs/ars/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>i</th>\n",
       "      <th>r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>943</td>\n",
       "      <td>871</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>943</td>\n",
       "      <td>1335</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>943</td>\n",
       "      <td>1342</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1367</th>\n",
       "      <td>943</td>\n",
       "      <td>1368</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392</th>\n",
       "      <td>943</td>\n",
       "      <td>1393</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11740</th>\n",
       "      <td>949</td>\n",
       "      <td>1655</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11742</th>\n",
       "      <td>949</td>\n",
       "      <td>1657</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11748</th>\n",
       "      <td>949</td>\n",
       "      <td>1663</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11749</th>\n",
       "      <td>949</td>\n",
       "      <td>1664</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11754</th>\n",
       "      <td>949</td>\n",
       "      <td>1669</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>230 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         u     i  r\n",
       "870    943   871  1\n",
       "1334   943  1335  1\n",
       "1341   943  1342  1\n",
       "1367   943  1368  1\n",
       "1392   943  1393  1\n",
       "...    ...   ... ..\n",
       "11740  949  1655  1\n",
       "11742  949  1657  1\n",
       "11748  949  1663  1\n",
       "11749  949  1664  1\n",
       "11754  949  1669  1\n",
       "\n",
       "[230 rows x 3 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df[:].groupby('u').apply(lambda x: print(x))\n",
    "treshold = 0.90\n",
    "attack_df_filterd = base_attack_df[base_attack_df.apply(lambda x: x.r > treshold, axis=1)] # apply on rows, keep only high prob ratings\n",
    "attack_df_filterd['r'] = attack_df_filterd['r'].apply(lambda x: 1) #|.astype(int)\n",
    "attack_df_filterd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 1097.53it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 6818.27it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([943, 943, 943, ..., 949, 949, 949]),\n",
       " array([1545, 1484, 1545, ..., 1431, 1420, 1396]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## add negative sampling to the mal data frame\n",
    "from tqdm import tqdm\n",
    "df = attack_df_filterd\n",
    "negative_items = {}\n",
    "users = list(sorted(df['u'].unique()))\n",
    "for idx, user in tqdm(enumerate(users), total=len(users)):\n",
    "    negative_items[user] = df[df['u'] != user]['i'].unique()\n",
    "negative_items\n",
    "num_negatives = 4\n",
    "df = df.groupby('u').apply(lambda s: s.sample(frac=1))\n",
    "user_input, item_input, labels = [], [], []\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    user = row['u']\n",
    "    user_input.append(user)\n",
    "    item_input.append(row['i'])\n",
    "    labels.append(row['r'])\n",
    "    negative_input_items = np.random.choice(negative_items[user], num_negatives)\n",
    "    for neg_item in negative_input_items:\n",
    "        user_input.append(user)\n",
    "        item_input.append(neg_item)\n",
    "        labels.append(0)\n",
    "\n",
    "mal_training_set = (np.array(user_input), np.array(item_input), np.array(labels))\n",
    "mal_training_set # this already has the inputs and negative items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_filterd 7188\n",
      "train_set 495285\n",
      "1150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50678"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Data import create_subset, concat_and_shuffle\n",
    "TRAIN_FRAC = 0.10\n",
    "malicious_training_set = mal_training_set\n",
    "# malicious_training_set = (df_filterd.values[:,0], df_filterd.values[:,1], df_filterd.values[:,2])\n",
    "print('df_filterd', len(df_filterd))\n",
    "print('train_set',len(train_set[0]))\n",
    "train_set_subset = create_subset(train_set, train_frac=TRAIN_FRAC)\n",
    "attack_benign_training_set = concat_and_shuffle(malicious_training_set, train_set_subset)\n",
    "print(len(malicious_training_set[0]))\n",
    "len(attack_benign_training_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model with: n_fake_users=8, n_users=943, n_movies=1682 created\n",
      "INFO:tensorflow:Restoring parameters from model.ckpt\n",
      "0.303 0.529 0.280 0.271\n",
      "0.303 0.528 0.279 0.243\n",
      "0.303 0.527 0.278 0.245\n",
      "0.303 0.522 0.277 0.247\n",
      "0.303 0.522 0.277 0.246\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model using the fake predictions\n",
    "tf.reset_default_graph()\n",
    "model = get_model(n_fake_users=N_FAKE_USERS, n_users=n_users, n_movies=n_movies)\n",
    "model_p = TFPredictWrapper(model)\n",
    "mal_epochs = 5\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, \"model.ckpt\")\n",
    "    for e in range(mal_epochs):\n",
    "        batches = get_batches(attack_benign_training_set, batch_size = batch_size)\n",
    "        u = attack_benign_training_set[0]\n",
    "        i = attack_benign_training_set[1]\n",
    "        r = attack_benign_training_set[2]\n",
    "        _, preds, loss = sess.run([model.train_op, model.prediction, model.loss],\n",
    "                                  feed_dict={model.user_inp: u,\n",
    "                                   model.item_inp: i,\n",
    "                                   model.y_true: r})\n",
    "            \n",
    "        epoch_loss = np.mean(loses_in_epoch)\n",
    "        mean_hr, mean_ndcg, time_eval = evaluate_model(model_p, test_set, verbose=0)\n",
    "        print('{:.3f} {:.3f} {:.3f} {:.3f}'.format(epoch_loss, mean_hr, mean_ndcg, time_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model with: n_fake_users=64, n_users=943, n_movies=1682 created\n",
      "INFO:tensorflow:Restoring parameters from model.ckpt\n",
      "0.491 0.552 0.283 0.301\n",
      "0.483 0.540 0.281 0.235\n",
      "0.481 0.534 0.275 0.232\n",
      "0.479 0.524 0.270 0.242\n",
      "0.476 0.493 0.258 0.240\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model using the fake predictions\n",
    "tf.reset_default_graph()\n",
    "model = get_model(n_fake_users=N_FAKE_USERS, n_users=n_users, n_movies=n_movies)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, \"model.ckpt\")\n",
    "    model_p = TFPredictWrapper(model)\n",
    "    for e in range(mal_epochs):\n",
    "        batches = get_batches(attack_benign_training_set, batch_size = batch_size)\n",
    "        loses_in_epoch = []\n",
    "        for b in batches:\n",
    "            u = b[:,0]\n",
    "            i = b[:,1]\n",
    "            r = b[:,2]\n",
    "            _, preds, loss = sess.run([model.train_op, model.prediction, model.loss],\n",
    "                                      feed_dict={model.user_inp: u,\n",
    "                                       model.item_inp: i,\n",
    "                                       model.y_true: r})\n",
    "            loses_in_epoch.append(np.mean(loss))\n",
    "        epoch_loss = np.mean(loses_in_epoch)\n",
    "        mean_hr, mean_ndcg, time_eval = evaluate_model(model_p, test_set, verbose=0)\n",
    "        print('{:.3f} {:.3f} {:.3f} {:.3f}'.format(epoch_loss, mean_hr, mean_ndcg, time_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with model.sess:\n",
    "new_inp.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(1, n_fake_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"FGSM ATTACK\"\"\"\n",
    "\n",
    "# user_inp = tf.placeholder(tf.float32, (n_fake_users, 64))\n",
    "# item_inp = tf.placeholder(tf.float32, (n_movies, 64))\n",
    "# # true_out = tf.placeholder(tf.float32, (n_fake_users, 10))\n",
    "epsilon = tf.placeholder(tf.float32, [n_fake_users * n_movies])\n",
    "# user_inp = tf.Variable((n_fake_users, 64), tf.float32)\n",
    "# item_inp = tf.Variable((n_fake_users, 64), tf.float32)\n",
    "# # true_out = tf.placeholder(tf.float32, (n_fake_users, 10))\n",
    "# epsilon = tf.Variable([n_fake_users, 64], tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    mean_hr, mean_ndcg, time_eval = evaluate_model(model_p, test_set, verbose=0)\n",
    "    preds = sess.run(model.prediction,\n",
    "                              feed_dict={model.user_inp: users,\n",
    "                               model.item_inp: items})\n",
    "    grad = tf.gradients(model.loss, [self.rating_input])\n",
    "# grad\n",
    "# grad1 = tf.gradients(loss, new_inp)\n",
    "# grad = tf.where(tf.is_nan(grad1), tf.zeros_like(grad1)*1, grad1) #replace nans with zeros\n",
    "# new_inp = tf.stop_gradient(data + epsilon * tf.sign(grad))\n",
    "# new_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"FGSM ATTACK\"\"\"\n",
    "\n",
    "n_adversarial = y_test.shape[0]\n",
    "\n",
    "trgt_inp = tf.placeholder(tf.float32, (n_fake_users, 64))\n",
    "# true_out = tf.placeholder(tf.float32, (n_fake_users, 10))\n",
    "epsilon = tf.placeholder(tf.float32, [n_adversarial, img_width, img_height, 1])\n",
    "\n",
    "new_inp = tf.identity(trgt_inp)  #returns a tensor with the same shape and type\n",
    "\n",
    "output = model(new_inp)\n",
    "\n",
    "loss = tf.sqrt(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=true_out))\n",
    "\n",
    "grad = tf.gradients(loss, new_inp)\n",
    "new_inp = tf.stop_gradient(new_inp + epsilon * tf.sign(grad))\n",
    "\n",
    "step = 0.1\n",
    "eps = np.full((n_adversarial, img_width, img_height, 1), step)\n",
    "\n",
    "feed_dict = {trgt_inp: x_test,\n",
    "             true_out: y_test,\n",
    "             epsilon: eps}\n",
    "\n",
    "x_adv, l = sess.run([new_inp, loss],\n",
    "                    feed_dict)\n",
    "\n",
    "x_adv = x_adv.reshape((n_adversarial, img_width, img_height, 1))\n",
    "\n",
    "acc_bfr_attck = model.evaluate(x=x_test, y=y_test, batch_size=32)[1]\n",
    "acc_aftr_attck = model.evaluate(x=x_adv, y=y_test, batch_size=32)[1]\n",
    "print('Accuracy before FGSM {}, Accuracy after FGSM {}'.format(acc_bfr_attck, acc_aftr_attck))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[2.]]\n",
    "m = tf.matmul(x, x)\n",
    "print(\"hello, {}\".format(m))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.zeros((10,))\n",
    "indexes = [1, 3, 5 ,7]\n",
    "t[indexes] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[indexes] = not t[indexes].all\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.43379653859233"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 0.000000000009\n",
    "-(1*np.log(p) + 0 * np.log(1-p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
